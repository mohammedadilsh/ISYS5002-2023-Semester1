{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfCtlWGnWNqAO0GfWE7+2W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isys5002-itp/ISYS5002-2023-Semester1/blob/main/2023_working_with_web.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download File/Data from the Web"
      ],
      "metadata": {
        "id": "71EBZxXa0QCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using `urllib`\n",
        "https://realpython.com/urllib-request/\n",
        "\n",
        "The docs [urllib.request.urlretrieve](https://docs.python.org/3/library/urllib.request.html#urllib.request.urlretrieve) state:\n",
        "\n",
        "> The following functions and classes are ported from the Python 2 module urllib (as opposed to urllib2). They might become deprecated at some point in the future.\n",
        "\n"
      ],
      "metadata": {
        "id": "u3YsZa9ymQ1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib \n",
        "from urllib import request\n",
        "urllib.request.urlretrieve('https://en.wikipedia.org/wiki/Python_(programming_language)', \"myFileURL.txt\")\n"
      ],
      "metadata": {
        "id": "ikCr3g3hmRVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using `requests`\n",
        "\n",
        "https://www.w3schools.com/python/module_requests.asp\n",
        "\n",
        "https://www.geeksforgeeks.org/downloading-files-web-using-python/\n"
      ],
      "metadata": {
        "id": "NCzwu5WljFZD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buijFrayZdi3"
      },
      "outputs": [],
      "source": [
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the requests library\n",
        "import requests\n",
        "\n",
        "site_url = \"https://en.wikipedia.org/wiki/Python_(programming_language)\"\n",
        "\n",
        "# sending get request and saving the response as response object\n",
        "response = requests.get(site_url) #use the get method to request a webpage/data from server\n",
        "\n",
        "# print content of response\n",
        "print(response.content)\n",
        "\n",
        "with open('myFileRequests.txt', 'wb') as file: # 'wb' because response stored as bytes\n",
        "  file.write(response.content)\n"
      ],
      "metadata": {
        "id": "7EZMZNN3ModC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using `wget`"
      ],
      "metadata": {
        "id": "IOVfYPGIjRHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "id": "Qj4Xdeu4eQ-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "\n",
        "# get a web page\n",
        "site_url ='https://en.wikipedia.org/wiki/Python_(programming_language)'\n",
        "\n",
        "# get a pdf file\n",
        "#site_url = 'https://link.springer.com/content/pdf/10.1007/s11306-019-1588-0.pdf?pdf=button%20sticky'\n",
        "\n",
        "file_name = wget.download(site_url)\n",
        "print(site_url)"
      ],
      "metadata": {
        "id": "iw0LHyk9fhzE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaccc2b2-a6eb-4c22-bba4-2ea025faf0e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://en.wikipedia.org/wiki/Python_(programming_language)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Summarising (Review)\n",
        "1. Installing Hugging Face Transformers\n",
        "2. Building a summarisation pipeline\n",
        "3. Run model/pipeline to summarisation\n",
        "4. Investigate way to reuse the pipeline\n",
        "> [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) free state-of-the-art pre-trained machine learning models for processing text, images, audio and video. See the project website for more information.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FHtmW79E_jJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Dependencies\n",
        "!pip install transformers -q"
      ],
      "metadata": {
        "id": "Ha0NwuodI4Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's make it a function - A function that take an 'article' and returns a summary"
      ],
      "metadata": {
        "id": "F1MWA01cL142"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "from transformers import pipeline\n",
        "\n",
        "def summarise(article):\n",
        "  summary_pipeline = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\") # load sumarisation pipeline \n",
        "  summary = summary_pipeline(article, max_length = 100, min_length= 50) # Run the summariser pipeline\n",
        "  text = summary[0]['summary_text'] # Extract the summarised text --- get first element, then extract the value for key 'summary text'\n",
        "  return text\n",
        "  "
      ],
      "metadata": {
        "id": "WW424_LrLwEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#call the summarise function\n",
        "some_text = '''\n",
        "A lack of transparency and reporting standards in the scientifc community has led to increasing and widespread\n",
        "concerns relating to reproduction and integrity of results. As an omics science, which generates vast amounts of data and\n",
        "relies heavily on data science for deriving biological meaning, metabolomics is highly vulnerable to irreproducibility. The\n",
        "metabolomics community has made substantial eforts to align with FAIR data standards by promoting open data formats,\n",
        "data repositories, online spectral libraries, and metabolite databases. Open data analysis platforms also exist; however,\n",
        "they tend to be infexible and rely on the user to adequately report their methods and results. To enable FAIR data science\n",
        "in metabolomics, methods and results need to be transparently disseminated in a manner that is rapid, reusable, and fully\n",
        "integrated with the published work. To ensure broad use within the community such a framework also needs to be inclusive\n",
        "and intuitive for both computational novices and experts alike\n",
        "'''\n",
        "from pprint import pprint\n",
        "pprint(summarise(some_text))"
      ],
      "metadata": {
        "id": "8ZaKqtWeMVyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting PDF using PyMuPDF (review)\n",
        "\n",
        "Refer to text_summerizer notebook for the steps"
      ],
      "metadata": {
        "id": "BO51cJoJOtFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the PDF file\n",
        "!wget https://link.springer.com/content/pdf/10.1007/s11306-019-1588-0.pdf"
      ],
      "metadata": {
        "id": "U2_dmnPNO5IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "id": "zsD5717PO69w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract text from pdf\n",
        "\n",
        "import fitz  # this is pymupdf\n",
        "\n",
        "file_name = \"/content/s11306-019-1588-0.pdf\"\n",
        "\n",
        "with fitz.open(file_name) as doc:\n",
        "    text = \"\"\n",
        "    for page in doc:        \n",
        "        text += page.get_text(\"text\")\n",
        "\n",
        "#print(text)"
      ],
      "metadata": {
        "id": "r_D9n_GYPItA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#summarizer pipeline have limit to 1024 words\n",
        "pprint(summarise(text[:1000])) # calling the def summarise(article) function"
      ],
      "metadata": {
        "id": "7OufJ97fa-eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Scraping & text summarisation\n",
        "\n",
        "## Scrape text from webpage and summarise text.\n",
        "Lets use the pipeline to summarise a web page. \n",
        "\n",
        "Google search and after looking at a few online articles, YouTube videos I settled on this page: [2 Ways to Extract Text From HTML Using Python](https://computersciencehub.io/python/extract-text-from-html-using-python/)\n",
        "\n",
        "https://computersciencehub.io/python/extract-text-from-html-using-python/\n",
        "\n",
        "https://www.geeksforgeeks.org/implementing-web-scraping-python-beautiful-soup/\n",
        "\n",
        "https://www.edureka.co/blog/web-scraping-with-python/"
      ],
      "metadata": {
        "id": "VdOrtHh1oSpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "\n",
        "# get webpage\n",
        "req = Request(\"https://en.wikipedia.org/wiki/Python_(programming_language)\")\n",
        "html_page = urlopen(req)\n",
        "\n",
        "soup = BeautifulSoup(html_page, features=\"html.parser\")\n",
        "\n",
        "# remove all 'script' and 'style' elements\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.extract()    # rip it out\n",
        "\n",
        "# get text\n",
        "text = soup.get_text()\n",
        "\n",
        "pprint(text)\n",
        "print('\\n\\n\\n')\n",
        "\n",
        "# break into lines and remove leading and trailing space on each\n",
        "lines =  text.splitlines()\n",
        "\n",
        "# remove empty lines\n",
        "lines = [x for x in lines if x]\n",
        "\n",
        "# combine into one body of text\n",
        "text = ' '.join(lines)\n",
        "# split into words\n",
        "text = text.split()\n",
        "# get first 400 words\n",
        "#text = text[:400]\n",
        "# join words into text\n",
        "text = ' '.join(text)\n",
        "\n",
        "\n",
        "pprint(text)"
      ],
      "metadata": {
        "id": "XMpiTNGgeQUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(summarise(text[:1000])) # calling the def summarise(article) function"
      ],
      "metadata": {
        "id": "AYfxENQecN_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's make it a function."
      ],
      "metadata": {
        "id": "iYRUaFaWyxky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "\n",
        "def summarise_webpage(URL):\n",
        "  ''' Sumarise the first 400 words on a website'''\n",
        "  # get webpage\n",
        "  req = Request(URL)\n",
        "  html_page = urlopen(req)\n",
        "  soup = BeautifulSoup(html_page, features=\"html.parser\")\n",
        "\n",
        "  # remove all 'script' and 'style' elements\n",
        "  for script in soup([\"script\", \"style\"]):\n",
        "      script.extract()    # rip it out\n",
        "\n",
        "  text = soup.get_text() # get text\n",
        "  lines =  text.splitlines() # break into lines\n",
        "  lines = [x for x in lines if x] # remove empty lines\n",
        "  text = ' '.join(lines) # combine into one body of text\n",
        "  text = text.split() # split into words\n",
        "  text = text[:400] # get first 400 words\n",
        "  text = ' '.join(text) # join words into text\n",
        "\n",
        "  return summarise(text)\n",
        "\n"
      ],
      "metadata": {
        "id": "pyXedLQyim88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = summarise_webpage(\"https://en.wikipedia.org/wiki/Python_(programming_language)\")\n",
        "print(text)"
      ],
      "metadata": {
        "id": "h00BMO0My40K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}